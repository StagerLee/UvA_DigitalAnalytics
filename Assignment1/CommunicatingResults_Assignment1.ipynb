{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about this template\n",
    "\n",
    "1. This Jupyter Notebook is a template that must be used for A1, as it is aligned with the grading criteria. Not following this template may have an impact on the grade. Please follow all the instructions included here. \n",
    "2. The appendix with the answers to the Data Ethics Decision Aid is to be handed in on a separate Canvas assignment, and in Word or PDF (not Jupyter notebook). See instructions on Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title of the Assignment\n",
    "\n",
    "* Student Name: NAME\n",
    "* Student Number: NUMBER\n",
    "* Date: DATE\n",
    "* Wordcount per section:\n",
    "  * Introduction:\n",
    "  * Hypotheses & Sub-RQs:\n",
    "  * Gathering data:\n",
    "  * Data Exploration & Evaluation:\n",
    "  * Evaluation:\n",
    "  * Limitations and Next Steps: \n",
    "  * Ethical and Normative Considerations:\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Doing data analysis is not just about crunching the numbers, but also explaining what is being done, and why. Moreover, it is important to document your steps in a way that other analysts (or even yourself) can understand what was done, with what type of data, and based on what assumptions.\n",
    "\n",
    "This section briefly explains the communication challenge and proposes a general RQ so that the reader has enough context to understand the actual analysis being done in the notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains the hypothesis (one) that will be tested by the study. The section (a) defines the key concepts for the hypothesis, and (b) provides a theoretical justification/motivation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data\n",
    "\n",
    "In the cells below you should load the dataset(s) into pandas to begin the data understanding and preparation. It is important to provide the reader with sufficient information to understand how the data were collected, and what the dataset is about. \n",
    "\n",
    "More specifically, you should answer at least adress the following topics in this section:\n",
    "* What is the dataset about? How does it relate to the business challenge?\n",
    "* What is the source of the data? \n",
    "    * If the data came from API's (e.g., Twitter, Facebook, YouTube) or specific tools (e.g., Google Analytics), this should be mentioned.\n",
    "    * If specific tools were used (e.g., DMI-TCAT for collecting Twitter data, or SentiStrength for sentiment analysis), they should also be mentioned (and the appropriate papers cited).\n",
    "* How were the data collected? What type of data collection (and/or sampling) strategy was used, and why? \n",
    "* The dataset is applicable to what time period? Why?\n",
    "* What are the potential biases that could be introduced in the dataset because of the data collection method?\n",
    "* How was privacy taken in consideration during data collection? What are some of the risks and/or privacy trade-offs that should be considered?\n",
    "\n",
    "For our assignment, if the dataset contains simulated data, we expect you to also include:\n",
    "* A clear disclaimer that this is a simulated dataset\n",
    "* A brief explanation of how the dataset was simulated\n",
    "* Important: the explanations and discussions should be written as if the dataset were real (i.e., indicating that “there are no privacy concerns because the dataset is simulated” is not acceptable).\n",
    "\n",
    "As a tip: use a meaningful name for your dataset(s) when loading them into a Pandas dataframe. Calling them just *df* or *data* will become confusing later on.\n",
    "\n",
    "**IMPORTANT:** As part of open science and replicability, it is important to indicate also to the reader where the data is stored or located. In the case of Assignment 1, we expect you to upload the dataset to our SurfDrive folder, so we can download the data and replicate your results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "As discussed during the tutorials, the stages of data understanding (which we call *Data Exploration & Evaluation* in the evaluation criteria) and data preparation (which we call *Data Cleaning* in the evaluation criteria) are iterative steps, with a lot of back and forth until you have a usable dataset.\n",
    "\n",
    "In this stage, we expect you to meet the criteria included in the course guide. While doing this, you need to make sure that you are also clearly communicating what is being done in each step. This includes:\n",
    "* Identifying the key variables that your study will use\n",
    "* Explaining to the reader what these variables are\n",
    "* Performing the steps regarding data cleaning and data exploration as indicated on Canvas (Assignment 2 briefing)\n",
    "* Explaining to the reader at each step what is being done, and what the output/result means\n",
    "\n",
    "**IMPORTANT:**\n",
    "* You only need to do the cleaning and exploration for the variables that are *relevant* to your study. You can skip variables/columns in the dataset that are absolutely irrelevant to your study.\n",
    "* You need to communicate your steps clearly to the reader. This means that you should not just do a *df.describe()*  to communicate descriptive statistics in a dataset that contains a lot of irrelevant columns and expect the reader to figure out by her or himself what is relevant. You should instead only show to the reader what is relevant, and explain why.\n",
    "* If you use functions to categorize your data, you should explain how the functions were built (or where they came from), and what is being done.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Evaluation\n",
    "\n",
    "In this stage, *after the dataset is considered clean*, you are expected to show to the reader how the final dataset looks like, and perform an exploratory review of the data. At all times, you are expected to write a report-out to stakeholders (i.e., not only show the data, but also explain what the data show).\n",
    "\n",
    "This includes:\n",
    "* Descriptives and definitions of all key variables in a clear and inteligible manner\n",
    "* Univariate visualizations for key variables \n",
    "* Visualization of key bivariate relationships (e.g., related to hypotheses or RQs)\n",
    "* Checking the data for biases and unbalance\n",
    "* Writing a report-out to stakeholders summarising the findings of the data exploration (including what the data already show when it comes to the RQ & hypothesis, and potential risks of bias and unbalance)\n",
    "\n",
    "Key variables are all variables that are used as DV's or IV's in your hypotheses or RQs.\n",
    "\n",
    "\n",
    "**IMPORTANT:**\n",
    "* You only need to do the exploration for the variables that are *relevant* to your study. You can skip variables/columns in the dataset that are absolutely irrelevant to your study.\n",
    "* You need to communicate your steps clearly to the reader. This means that you should not just do a *df.describe()*  to communicate descriptive statistics in a dataset that contains a lot of irrelevant columns and expect the reader to figure out by her or himself what is relevant. You should instead only show to the reader what is relevant, and explain why.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Hypothesis Testing\n",
    "\n",
    "After understanding and preparing the data, you are ready to do the modeling. It is important to explain to the reader:\n",
    "* Explaining which models will be used, and why\n",
    "* If appropriate, explaining which strategy you are using to fine tune or improve the model\n",
    "* Discussion about the model evaluation (i.e., why you believe the model is a good model) and, if appropriate, comparison between different models\n",
    "* After the appropriate model is identified, then the hypothesis can be tested. You can then report the result of the tests being used to test the hypothesis (as discussed in the criteria for the assignment), explaining to the reader how the test is being made and what the results of the testing mean for the hypothesis at hand. \n",
    "* Visualizations (as appropriate) are helpful to explain to the reader what has been found.\n",
    "* Usage of the model to make predictions (e.g., key audience segments)\n",
    "* Reviewing the model to indicate which features (variables) are responsible for the predictions (related to Explainable AI discussions)\n",
    "\n",
    "**IMPORTANT:**\n",
    "* We consider as different models both when different features/variables are being tested (to achieve the same objective), or when different algorithms are being tested (with the same features/IVs)\n",
    "* It is important to explain to the reader what the model contains/is, and also how it is being evaluated. When the evaluation is done, it is also important to discuss what the result means in the current context\n",
    "* As the focus of this course is on predictive analytics, it is expected that predictions are made (using scikit-learn) and discussed after a hypothesis is tested. This means that when ANOVAs or T-Tests are used (as statistical testing), and equivalent OLS/Linear Regression Model (in scikit-learn) needs to be used to make predictions. All tests need to be done in Python, using statsmodels and/or scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation \n",
    "\n",
    "This section provides the answer to the communication challenge, discussing the results (hypothesis testing) in order to answer the general RQ. Creativity in how to summarize the findings (including visualizations) is a plus.\n",
    "\n",
    "This section also provides a set of implications (i.e., now that we know the answer to the RQ, what does it mean for the organization/process/challenge? what should the organization do?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations and Next Steps\n",
    "\n",
    "In this section you should discuss all the relevant limitations. This includes the limitations of the analysis and of the data, thus consolidating and extending the discussion already included in the earlier sections (e.g., Data Collection, Data Exploration and Modelling), and what do these limitations mean to the conclusions and to the overall challenge.\n",
    "\n",
    "In other words, make sure that you are at least covering:\n",
    "* The dataset(s) itself, including how the data collection may have created limitations\n",
    "* Limitations associated with the decisions taken during the data understanding & preparation\n",
    "* Limitations about the model and hypothesis testing (including fit, selection of variables)\n",
    "* Alternative interpretations to the findings\n",
    "* You need also to suggest next steps/actions to overcome the most important limitations (as indicated in the assignment criteria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical and Normative Considerations\n",
    "\n",
    "This section should discuss the considerations the organization needs to have. Are there ethical or more general normative aspects that need to be taken into account? Drawing directly from the literature discussed in class (and other literature), what should readers of the data analysis be mindful about? Make sure to cite the relevant literature. \n",
    "\n",
    "This discussion can relate to the actual dataset/analysis that was done, to the recommendations/implications, and to the action plan suggested in the previous sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "If you use scientific papers, cite them using APA style (and add a reference section at the end). If you are using pieces of code written by someone else, add a comment in the appropriate section and add a link to the source.\n",
    "\n",
    "Regarding the tools we used:\n",
    "* Sentiment analysis was done using SentiStrength. The following paper can be cited:\n",
    "    * Thelwall, M., Buckley, K., Paltoglou, G., Cai, D., & Kappas, A. (2010). Sentiment strength detection in short informal text. *Journal of the American Society for Information Science and Technology*, 61(12), 2544-2558.\n",
    "\n",
    "* Tweets were collected using DMI-TCAT. The following paper can be cited:\n",
    "    * Borra, E., & Rieder, B. (2014). Programmed method: developing a toolset for capturing and analyzing tweets. *Aslib Journal of Information Management*, 66(3), 262-278.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
